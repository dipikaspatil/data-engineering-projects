{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778cd2ab-8211-4195-9726-6b4ed5dd637a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /opt/homebrew/Cellar/jupyterlab/4.4.5/libexec/lib/python3.13/site-packages (from pyspark) (0.10.9.9)\n"
     ]
    }
   ],
   "source": [
    "# Install PySpark\n",
    "import sys\n",
    "!{sys.executable} -m pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8eddc6-8060-4170-9584-6b87c7c7b863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# Verify Installation\n",
    "import pyspark\n",
    "print(\"PySpark version:\", pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c638a91f-d16a-49e7-922e-56bb3fadd7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.16\" 2025-07-15\n",
      "OpenJDK Runtime Environment Homebrew (build 17.0.16+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 17.0.16+0, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06e63c1-e424-48ce-b425-946d3ed06236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/02 16:57:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created!\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 1. Import Spark and Start Session\n",
    "# -----------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieLensEDA_LoadInspect\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1b64cea-3ffb-4eea-9fc6-dd190979c20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings Schema:\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n",
      "Sample Ratings:\n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|   196|    242|     3|881250949|\n",
      "|   186|    302|     3|891717742|\n",
      "|    22|    377|     1|878887116|\n",
      "|   244|     51|     2|880606923|\n",
      "|   166|    346|     1|886397596|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 2. Load Ratings Data\n",
    "# -----------------------------------\n",
    "# File: u.data --> userId, movieId, rating, timestamp\n",
    "ratings = spark.read.csv(\"../data/u.data\",\n",
    "                         sep=\"\\t\",\n",
    "                         header=False,\n",
    "                         inferSchema=True) \\\n",
    "    .toDF(\"userId\", \"movieId\", \"rating\", \"timestamp\")\n",
    "\n",
    "print(\"Ratings Schema:\")\n",
    "ratings.printSchema()\n",
    "\n",
    "print(\"Sample Ratings:\")\n",
    "ratings.show(5)\n",
    "\n",
    "# Notes:\n",
    "# .toDF(\"userId\", \"movieId\", \"rating\", \"timestamp\") → Renames the columns after reading, because the file doesn’t have headers.\n",
    "# ratings is now a PySpark DataFrame with 4 columns: userId, movieId, rating, timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7acb19d7-2454-4cf4-8358-1d189abda28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies Schema:\n",
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "Sample Movies:\n",
      "+-------+-----------------+\n",
      "|movieId|            title|\n",
      "+-------+-----------------+\n",
      "|      1| Toy Story (1995)|\n",
      "|      2| GoldenEye (1995)|\n",
      "|      3|Four Rooms (1995)|\n",
      "|      4|Get Shorty (1995)|\n",
      "|      5|   Copycat (1995)|\n",
      "+-------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 3. Load Movies Data\n",
    "# -----------------------------------\n",
    "# File: u.item --> movieId | title | other metadata...\n",
    "movies = spark.read.csv(\"../data/u.item\",\n",
    "                        sep=\"|\",\n",
    "                        header=False,\n",
    "                        inferSchema=True) \\\n",
    "    .selectExpr(\"_c0 as movieId\", \"_c1 as title\")\n",
    "\n",
    "print(\"Movies Schema:\")\n",
    "movies.printSchema()\n",
    "\n",
    "print(\"Sample Movies:\")\n",
    "movies.show(5)\n",
    "\n",
    "# Notes:\n",
    "#.selectExpr(\"_c0 as movieId\", \"_c1 as title\") → Selects only the first two columns:\n",
    "    # _c0 → renamed to movieId\n",
    "    #_c1 → renamed to title\n",
    "#movies DataFrame contains 2 columns only: movieId and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fe152cd-d07a-48c3-93e5-ef085a919f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Ratings: 100000\n",
      "Total Movies: 1682\n",
      "Total Users: 943\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 4. Basic Dataset Stats\n",
    "# -----------------------------------\n",
    "num_ratings = ratings.count()\n",
    "num_movies = movies.count()\n",
    "num_users = ratings.select(\"userId\").distinct().count()\n",
    "\n",
    "print(f\"Total Ratings: {num_ratings}\")\n",
    "print(f\"Total Movies: {num_movies}\")\n",
    "print(f\"Total Users: {num_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4d90d8d-4b47-4e6c-a5b2-1e738de2cf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# 5. Stop Spark Session\n",
    "# -----------------------------------\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b29fe-8860-4f27-8d0b-2eb78612d220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
